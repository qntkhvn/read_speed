---
title: Analysis of CSV File Reading in R
author:
  - Quang Nguyen
  - Robert Tedesco
date: STAT 407 Fall 2021
output: 
  pdf_document: 
    number_sections: true
    df_print: kable
# abstract: "write some stuff"
header-includes:
 \usepackage{setspace}
 \setlength{\parskip}{3mm}
 \onehalfspacing
 \usepackage{float}
 \floatplacement{figure}{H}
 \usepackage[skip=3pt]{caption}
bibliography: ref.bib
csl: apa.csl
indent: true
geometry: margin=1.1in
fontsize: 12pt
---

\begin{center}

\small

Abstract

\end{center}

\vspace{-8mm}

\small

> In this report, we do blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah

\normalsize

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE,
                      echo = FALSE,
                      warning = FALSE,
                      fig.pos = "H")
```

```{r}
library(tidyverse)
theme_set(theme_bw())
library(patchwork)
library(broom)
library(car)
library(rcompanion)
```

```{r}
res <- bind_rows(read_csv("res_quang.csv"),
                 read_csv("res_rob.csv"),
                 read_csv("res_lance.csv")) %>% 
  mutate(computer = str_replace(computer, "lance", "dell"),
         computer = str_replace(computer, "rob", "lenovo"),
         computer = str_replace(computer, "quang", "mac"),
         
         across(where(is_character), factor))
```


\newpage

# Introduction

Data importing is undoubtedly a crucial part of a modern statistics and data science workflow, as it is the first step that paves the path for important stages such as data wrangling, visualizing, and modeling. With modern computers, loading data may seem like a straightforward task for a statistician. However, as the volume of data increases, this imposes a new set of challenges related to computing time and efficiency for data importing. A researcher may have to load a large data file multiple times a week in order to perform statistical analyses, which could potentially cost them valuable time. Identifying the fastest computing techniques is a necessity in the computational problem of working with big data.

The `R` programming language [@baser] is well-known among statisticians and data scientists as a powerful tool for working with data. In `R`, a popular method used by traditional statisticians to load a comma-separated values (CSV) file into the environment is `read.csv()`, which is a built-in base function. However, there are some drawbacks with this method, when working with large data sets. As demonstrated by @gregcsv, it took minutes to import a CSV file of about two-gigabyte containing over a million rows of NFL tracking data with the historical `read.csv()` function in base `R`. Fortunately, in recent years, `R` developers have come up with better ways to read in data files. As mentioned by @efficient in their "Efficient R programming" book, there are three modern functions/packages that `R` users should take advantage of in order to improve file-reading performance. 

The first function that `R` programmers should consider is `import()`, which comes from the `rio` package [@rio]. As described by its creators in the package documentation, `rio` is  "a swiss-army knife for data I/O".  The function `import()` in `rio` essentially simplifies the data importing process since as a function by itself, `import()` has the flexibility of reading in multiple file extensions, such as `.json`, `.xls`, `.xlsx`, in addition to the common `.csv`.  The data is stored as a `data.frame` object after being loaded into `R` in using `import()`.

Another `R` package for importing data that has gotten a lot of attention recently is `readr` [@readr], which is also part of the popular `tidyverse` collection of packages [@tidyverse] for modern data science. Within `readr`, there is a `read_csv()` function for getting CSV files into `R`, alongside other functions of the `read_*()` family designed for other file extensions. A notable aspect of the functions in `readr` in general and `read_csv()` in particular is the data is stored as a `tibble` (a "modern" `data.frame`-typed object) in the `R` environment once it is read in.

The third and final package for efficient file reading in `R` is `data.table` [@datatable], which contains the `fread()` function for data importing. As an individual function, `fread()` is also capable of loading a variety of file extensions, similar to `rio::import()`. As for object type, the `fread()` function imports and returns a data object of classes `data.table` and `data.frame`.

In this paper, we attempt to design a three-factor factorial experiment with replicates and perform analyses to compare the speed of the three CSV importing algorithms in `R` as mentioned in previous paragraphs. In particular, the independent variables we are interested in examining are 1) the `R` packages/functions; 2) the file size; and 3) the computer device. The paper is outlined as follows. We first describe our data generating process for our experiment and the methodologies used for analyses in Section 2. We then spend the next section, 3, on our analyses and results of this experiment. Lastly, in Section 4, we give a quick summary of our results as well as discuss possible future work related to this project.

# Experiment

The goal of this analysis is to determine the effects of three factors - `R` package, file size, computer device - on the reading time for CSV files measured in seconds. Because of this, we chose to design our experiment with a  $3^3$ factorial experiment [@montgomery] in mind. Furthermore, we chose to include $n=20$ replicates for each combination of our factors.  Our chosen design implies that the total number of observations for our experiment is 540. Our data was generated by considering the $3^3$ possible combinations of `R` package, file size, and computer. We then measured the twenty replicates for each combination according to the order of a random sample of 1 through $3^3$.  

The three levels for each of our factors are described as follows. For our first factor, R functions and their associated packages, we considered the three package-function combinations as introduced in Section 1. This includes  `rio::import()`, `data.table::fread()`, and `readr::read_csv()`. As for our second variable, file size, we utilized `R` to generate and export three different CSV files of different sizes. Each data file consists of columns of randomly-draw samples from a standard normal distribution. First, the largest file consists of 1000000 rows and 100 columns and has a size of 0.49 GB. Next, we have a mid-sized file of 1.22 GB on disk containing 1000000 cases and 62 variables. Finally, our smallest CSV file has a row-by-column dimension of 252000 by 100 and takes up 1.96 GB of storage. Last but not least, we considered three different laptop devices for this experiment. In particular, we have a MacBook Air 2020 with an M1 processor, a Lenovo Legion 2019 with a Ryzen 7 4800H processor, and a Dell XPS 13 2020 with an  Intel i7 processor.

# Analysis of Variance

In this section, we present the analysis of variance for our designed experiment. First and foremost, in order to gain a better understanding of our data, we used interaction plots (see Figure 1) to observe that reading time varies for combinations of each factor. Interestingly, reading time seemed to increase significantly for all three packages when working with the largest data file (~2GB). `readr` seems to take longer than the other two packages considered when working with large data files. The Lenovo appeared to be faster than the Macbook and the Dell for all file sizes- the interaction does not appear significant. Lastly, the interaction of computer and package appears to be significant - the Macbook reads data quicker than the other computers using the `readr` package.

```{r}
p1 <- res %>% 
  group_by(size, package) %>% 
  summarize(avg_time = mean(time)) %>% 
  ungroup() %>% 
  ggplot(aes(size, avg_time)) +
  geom_line(size = 1, aes(group = package, color = package)) +
  geom_point(size = 2.5, aes(color = package), shape = 15) +
  theme(legend.position = "bottom")

p2 <- res %>% 
  group_by(size, computer) %>% 
  summarize(avg_time = mean(time)) %>% 
  ungroup() %>% 
  ggplot(aes(size, avg_time)) +
  geom_line(size = 1, aes(group = computer, color = computer)) +
  geom_point(size = 2.5, aes(color = computer), shape = 15) +
  theme(legend.position = "bottom")

p3 <- res %>% 
  group_by(package, computer) %>% 
  summarize(avg_time = mean(time)) %>% 
  ungroup() %>% 
  ggplot(aes(package, avg_time)) +
  geom_line(size = 1, aes(group = computer, color = computer)) +
  geom_point(size = 2.5, aes(color = computer), shape = 15) +
  theme(legend.position = "bottom")
```

```{r, fig.width=12, fig.height=4.5, fig.cap="Interaction plots between the variables size, computer, and package"}
p1 + p2 + p3
```

After gaining an understanding of our data, the next step was to fit the model corresponding to our experiment. The model for a three-level factorial experiment with three factors is given by

\begin{equation}
y_{ijkl} = \mu + \tau_i + \beta_j + \gamma_k + (\tau \beta)_{ij} + (\tau \gamma)_{ik} + (\beta \gamma)_{jk} + (\tau \beta \gamma)_{ijk} + \epsilon_{ijkl},
\end{equation}
where $i, j , k = 1, 2, 3$ and $l = 1, 2, ...,20$, since we have $n=20$ replicates.

The response variable, $y_{ijkl}$, indicates the speed time in seconds corresponding to each run conducted in the experiment. Here $\mu$ represents the overall mean reading time,  and $\tau_i$, $\beta_j$, and $\gamma_k$ are the main effects of three factors file size, computer device, and `R` function/package. There are also 3 two-factor and 1 three-factor interaction effects in our model. The assumptions for the model are that the residuals $\epsilon_{ijkl}$ are normally distributed and that the variance is constant across the groups.

Using this model, we can conduct an ANOVA to see whether the levels in each factor significantly differ in mean data loading speed time. In addition, we can test for significance of each of the specified interaction effects. The model produced significant results (see Table 1), as all the terms in the ANOVA fit appear to have an effect on the response variable, data importing time. However, the residual assumptions are not met for this model, as illustrated by Figure 2 and Table 2. In particular, a normal probability plot followed by a Shapiro-Wilk test indicated a significant departure from normality for our model residuals. Moreover, a residuals versus fitted values plot and a Levene Test showed a violation of the equality of variances condition.

```{r}
mod <- lm(time ~ computer * size * package, data = res)
mod %>% 
  anova() %>% 
  tidy() %>% 
  mutate(across(where(is.numeric), ~ round(.x, 3))) %>% 
  knitr::kable(caption = "ANOVA for the three-factor factorial ANOVA model.")
```

```{r}
pqq <- tibble(resid = resid(mod)) %>% 
  ggplot(aes(sample = resid)) +
  stat_qq() + 
  stat_qq_line(linetype = "dashed")

prvf <- tibble(resid = resid(mod),
             fitted = fitted(mod)) %>% 
  ggplot(aes(fitted, resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed")
```

```{r, fig.width=6, fig.height=3, fig.cap="Residuals plots"}
pqq + prvf
```

```{r}
shap <- mod %>% 
  pluck("residuals") %>% 
  shapiro.test() %>% 
  tidy()
```

```{r}
lev <- leveneTest(time ~ computer * size * package, data = res) %>% 
  tidy() %>% 
  dplyr::select(-contains("df")) %>% 
  mutate(method = "Levene test for homogeneity of variance")
```

```{r}
shap %>% 
  bind_rows(lev) %>% 
  mutate(across(where(is.numeric), ~ round(.x, 3)),
         p.value = "0.000") %>% 
  knitr::kable(caption = "Shapiro-Wilk and Levene tests for the three-factor factorial ANOVA model.")
```

As a result of violations of normality and homoscedasticity, we then considered a couple of data transformation methods, namely, a log transformation and a Box-Cox power transformation [@boxcox] as possible ways to resolve the non-normality issue of our data. However, transforming the response variable, file reading time, did not improve the fit of this model and our assumptions are still not met. The ANOVA table, visual summary, and tests for normality and constant variance for these two methods can be found in the Appendix.

This then led us to look into a non-parametric alternative to the $3^3$ factorial model. We ended up using a permutation test for a three-way design, which does not assume a normal distribution for our data, and this is implemented via the function `perm.fact.test()` in the `asbio` `R` package [@asbio]. Table 3 shows the computer output for the permutation test for our three-factor factorial experiment. As we can see, since each factor and all interaction effects are significant, statisticians have a lot to keep in mind as they work with big data; certain machines may excel with certain packages and what package is best for reading data depends on the size of the file at hand. 

```{r}
perm.fact.test <- function(Y,
                           X1,
                           X2,
                           X3 = NA,
                           perm = 100,
                           method = "a") {
  if (all(is.na(X3))) {
    init.model <- anova(lm(Y ~ X1 * X2))
  }
  if (all(!is.na(X3))) {
    init.model <- anova(lm(Y ~ X1 * X2 * X3))
  }
  
  r <- length((init.model)$"F value") - 1
  F.init <- init.model$"F value"[1:r]
  MS <- init.model$"Mean Sq"
  
  # (a)
  if (method == "a") {
    F.perm <- matrix(nrow = r, ncol = perm)
    if (all(is.na(X3))) {
      for (i in 1:perm) {
        Y.new <- sample(Y, replace = FALSE)
        F.perm[, i] <- anova(lm(Y.new ~ X1 * X2))$"F value"[1:r]
      }
    }
    
    if (all(!is.na(X3))) {
      for (i in 1:perm) {
        Y.new <- sample(Y, replace = FALSE)
        F.perm[, i] <- anova(lm(Y.new ~ X1 * X2 * X3))$"F value"[1:r]
      }
    }
  }
  
  
  # (b)
  if (method == "b") {
    MS.perm <- matrix(nrow = r + 1, ncol = perm)
    if (all(is.na(X3))) {
      for (i in 1:perm) {
        Y.new <- sample(Y, replace = FALSE)
        MS.perm[, i] <- anova(lm(Y.new ~ X1 * X2))$"Mean Sq"
      }
    }
    
    if (all(!is.na(X3))) {
      for (i in 1:perm) {
        Y.new <- sample(Y, replace = FALSE)
        MS.perm[, i] <- anova(lm(Y.new ~ X1 * X2 * X3))$"Mean Sq"
      }
    }
    
    F.perm <- matrix(nrow = r, ncol = perm)
    for (i in 1:perm) {
      F.perm[, i] <- MS.perm[, i][1:r] / MS.perm[, i][r + 1]
    }
  }
  
  p1 <- matrix(nrow = r, ncol = perm)
  for (i in 1:r) {
    p1[i, ] <- F.perm[i, ] >= F.init[i]
  }
  
  p2 <- apply(p1, 1, function(x) {
    length(x[x == TRUE])
  })
  
  p.val <- (p2 + 1) / perm
  p.val <- ifelse(p.val > 1, 1, p.val)
  
  if (all(is.na(X3))) {
    Table <-
      data.frame(
        Initial.F = init.model$"F value",
        Df = init.model$"Df",
        row.names = c("X1", "X2", "X1:X2", "Residual"),
        pval = c(p.val, NA)
      )
  }
  if (all(!is.na(X3))) {
    Table <-
      tibble(
        Term = c(
          "X1",
          "X2",
          "X3",
          "X1:X2",
          "X1:X3",
          "X2:X3",
          "X1:X2:X3",
          "Residual"
        ),
        InitialF = init.model$"F value",
        DF = init.model$"Df",
        pval = c(p.val, NA)
      )
  }
  res <- list()
  res$Table <- Table
  res
}

set.seed(69)
perm <- perm.fact.test(Y = res$time,
                       X1 = res$size,
                       X2 = res$computer,
                       X3 = res$package,
                       perm = 9999)

perm$Table %>% 
  mutate(Term = str_replace(Term, "X1", "size"),
         Term = str_replace(Term, "X2", "computer"),
         Term = str_replace(Term, "X3", "package")) %>% 
  mutate(across(where(is.numeric), ~ round(.x, 3))) %>% 
  knitr::kable(caption = "Permutation test for the three-factor factorial ANOVA model.")
```

We then performed a pairwise permutation test to determine which pairs of levels in each of our three factors (file size, computer device, and function/package) differ significantly in terms of average file reading time. This was implemented using the `pairwisePermutationTest() ` function in the `rcompanion` `R` package. We observed that the two pairs of functions `data.table::fread()` - `rio::import()` and `rio::import()` - `readr::read_csv()` have different mean file importing time.  As for computer devices, Dell and MacBook don’t differ in data loading time, but they each have different mean reading times compared to Lenovo. Lastly, there is a difference in file importing time for all pairs of file sizes.

```{r}
pw1 <- pairwisePermutationTest(time ~ size, data = res, method = "fdr")
pw2 <- pairwisePermutationTest(time ~ computer, data = res, method = "fdr") 
pw3 <- pairwisePermutationTest(time ~ package, data = res, method = "fdr")

as_tibble(pw1) %>% 
  bind_rows(as_tibble(pw2)) %>% 
  bind_rows(as_tibble(pw3)) %>% 
  mutate(factor = c(rep("size", 3), rep("computer", 3), rep("package", 3))) %>% 
  select(factor, comparison = Comparison, statistic = Stat, p.value, p.adjust) %>% 
  mutate(across(where(is.numeric), ~ round(.x, 4))) %>% 
  knitr::kable(caption = "Pairwise comparisons using permutation tests with FDR adjustment for the three-factor factorial ANOVA model.")
```

# Additional Analysis

One particularly interesting observation we had after conducting analysis of variance to our data is that  `readr::read_csv()` is the fastest method when running on the MacBook device, but on the other hand, the slowest for the other two computers.  Thus, we are interested in performing a quick simulation study to see if there is a function/package effect on the file reading time for the MacBook device only. In addition to the three data files with sizes 0.49, 1.22, 1.96 GB used in the previous experiment, we simulated seven additional datasets with sizes 1.51, 1.73, 2.16, 2.36, 2.61, 2.94, 3.32 GB

We then conducted an analysis of covariance (ANCOVA) with time elapsed as the response variable,  the file reading function as the categorical independent variable, and the file size as the continuous covariate. 

The table output suggests that the mean reading time differs for all `R` CSV reading functions, after accounting for the effect of file size. Furthermore, both normality and constant variance conditions are met for this model, as illustrated by a Shapiro-Wilk test for normality and a Levene Test for homogeneity of variance.

We can then proceed to perform multiple comparisons to see which algorithms are different from each other. It is confirmed that the mean reading time for `readr::read_csv()` is different (faster) than the mean reading time of the other two techniques, and the other two also don’t differ in terms of average importing time. 


# References {-}

<div id="refs"></div>

\newpage

# Appendix {-}

log transfomation

```{r}
mod <- lm(log(time) ~ computer * size * package, data = res)
mod %>% 
  anova() %>% 
  tidy() %>% 
  mutate(across(where(is.numeric), ~ round(.x, 3))) %>% 
  knitr::kable(caption = "ANOVA for the three-factor factorial ANOVA model.")
```

```{r}
pqq <- tibble(resid = resid(mod)) %>% 
  ggplot(aes(sample = resid)) +
  stat_qq() + 
  stat_qq_line(linetype = "dashed")

prvf <- tibble(resid = resid(mod),
             fitted = fitted(mod)) %>% 
  ggplot(aes(fitted, resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed")
```

```{r, fig.width=6, fig.height=3, fig.cap="Residuals plots"}
pqq + prvf
```

```{r}
shap <- mod %>% 
  pluck("residuals") %>% 
  shapiro.test() %>% 
  tidy()
```

```{r}
lev <- leveneTest(log(time) ~ computer * size * package, data = res) %>% 
  tidy() %>% 
  dplyr::select(-contains("df")) %>% 
  mutate(method = "Levene test for homogeneity of variance")
```

```{r}
shap %>% 
  bind_rows(lev) %>% 
  mutate(across(where(is.numeric), ~ round(.x, 3)),
         p.value = "0.000") %>% 
  knitr::kable(caption = "Shapiro-Wilk and Levene tests for the three-factor factorial ANOVA model.")
```

Box-Cox Transformation -0.2543644

```{r}
mod <- lm(time ^ (-0.2543644) ~ computer * size * package, data = res)
mod %>% 
  anova() %>% 
  tidy() %>% 
  mutate(across(where(is.numeric), ~ round(.x, 3))) %>% 
  knitr::kable(caption = "ANOVA for the three-factor factorial ANOVA model.")
```

```{r}
pqq <- tibble(resid = resid(mod)) %>% 
  ggplot(aes(sample = resid)) +
  stat_qq() + 
  stat_qq_line(linetype = "dashed")

prvf <- tibble(resid = resid(mod),
             fitted = fitted(mod)) %>% 
  ggplot(aes(fitted, resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed")
```

```{r, fig.width=6, fig.height=3, fig.cap="Residuals plots"}
pqq + prvf
```

```{r}
shap <- mod %>% 
  pluck("residuals") %>% 
  shapiro.test() %>% 
  tidy()
```

```{r}
lev <- leveneTest(time ^ (-0.2543644) ~ computer * size * package, data = res) %>% 
  tidy() %>% 
  dplyr::select(-contains("df")) %>% 
  mutate(method = "Levene test for homogeneity of variance")
```

```{r}
shap %>% 
  bind_rows(lev) %>% 
  mutate(across(where(is.numeric), ~ round(.x, 3)),
         p.value = "0.000") %>% 
  knitr::kable(caption = "Shapiro-Wilk and Levene tests for the three-factor factorial ANOVA model.")
```
